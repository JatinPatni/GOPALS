{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365e5672-9484-444f-b9ca-2524a2e21362",
   "metadata": {},
   "source": [
    "### Here we will generate new market scenarios by introducing noise in historical returns, risks and correlations. \n",
    "\n",
    "### Remember that we will try to find stable regions in portfolio weight space which perform consistently across these simulated market scenarios.\n",
    "\n",
    "### The Parameters we will use are:\n",
    "- Noise of 20, 30, 50% on returns\n",
    "- Noise of 10, 20% on risks\n",
    "- Noise of 5, 10% on correlations\n",
    "\n",
    "These are 3 * 2 * 2 = 12 combinations\n",
    "\n",
    "We will simulate 500 random scenarios for each of these combinations to get a total of 6000 scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89386098-875d-429b-a55d-623384c2b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee00a4c-b191-4672-bc21-23177b49963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_dir = r'../data/simulations/market_scenarios_historical'\n",
    "output_data_dir = r'../data/simulations/market_scenarios_monte_carlo'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d609717c-9661-4abd-ae73-e85cdd4731ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 10 5\n",
      "20 10 10\n",
      "20 20 5\n",
      "20 20 10\n",
      "35 10 5\n"
     ]
    }
   ],
   "source": [
    "ret_noise_list = [20, 35, 50]\n",
    "risk_noise_list = [10,20]\n",
    "corr_noise_list = [5,10]\n",
    "\n",
    "# tyear = 2017\n",
    "# ret_noise = 20\n",
    "\n",
    "# risk_noise = 10\n",
    "# corr_noise = 5\n",
    "combinations = list(itertools.product(ret_noise_list, risk_noise_list, corr_noise_list))\n",
    "for a,b,c in combinations[:5]:\n",
    "    print(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11e1b1bd-2402-4d48-8dfb-7c55d62dc821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [01:19<00:00,  9.97s/it]\n"
     ]
    }
   ],
   "source": [
    "year_list = list(range(2017, 2025))\n",
    "ret_noise_list = [20, 35, 50]\n",
    "risk_noise_list = [10,20]\n",
    "corr_noise_list = [5,10]\n",
    "combinations = list(itertools.product(ret_noise_list, risk_noise_list, corr_noise_list))\n",
    "\n",
    "sim_num = 1\n",
    "total_sim = 100\n",
    "for tyear in tqdm(year_list):\n",
    "    for ret_noise, risk_noise, corr_noise in combinations:\n",
    "        for i in range(total_sim):\n",
    "            \n",
    "            ret_df = pd.read_csv(os.path.join(input_data_dir, str(tyear)+'_annual_ret.csv'), header=None)\n",
    "            ret_df[1]=ret_df[1] * np.random.choice([1+0.01*ret_noise, 1-0.01*ret_noise], size=len(ret_df))\n",
    "            ret_df.to_csv(os.path.join(output_data_dir, 'mc_sim_'+str(sim_num) +'_year_'+ str(tyear) + '_ret_noise_' + str(ret_noise)+ '.csv'), index=False, header=False)\n",
    "            \n",
    "            risk_df = pd.read_csv(os.path.join(input_data_dir, str(tyear)+'_annual_vol.csv'), header=None)\n",
    "            risk_df[1]=risk_df[1] * np.random.choice([1+0.01*risk_noise, 1-0.01*risk_noise], size=len(risk_df))\n",
    "            risk_df.to_csv(os.path.join(output_data_dir, 'mc_sim_'+str(sim_num) +'_year_'+ str(tyear) + '_risk_noise_' + str(risk_noise)+ '.csv'), index=False, header=False)\n",
    "            \n",
    "            corr_df = pd.read_csv(os.path.join(input_data_dir, str(tyear)+'_annual_ret.csv'), header=None)\n",
    "            ret_df[1]=ret_df[1] * np.random.choice([1+0.01*ret_noise, 1-0.01*ret_noise], size=len(ret_df))\n",
    "            ret_df.to_csv(os.path.join(output_data_dir, 'mc_sim_'+str(sim_num) +'_year_'+ str(tyear) + '_ret_noise_' + str(ret_noise)+ '.csv'), index=False, header=False)\n",
    "            \n",
    "            corr_df = pd.read_csv(os.path.join(input_data_dir, str(tyear)+'_annual_corr.csv'), index_col=0)\n",
    "            correlation_matrix = corr_df.values\n",
    "            random_multipliers = np.random.choice([1+0.01*corr_noise, 1-0.01*corr_noise], size=correlation_matrix.shape)\n",
    "            np.fill_diagonal(random_multipliers, 1)\n",
    "            modified_correlation_matrix = correlation_matrix * random_multipliers\n",
    "            \n",
    "            modified_correlation_df = pd.DataFrame(modified_correlation_matrix, \n",
    "                                                   index=corr_df.index, \n",
    "                                                   columns=corr_df.columns)\n",
    "            modified_correlation_df.to_csv(os.path.join(output_data_dir, 'mc_sim_'+str(sim_num) +'_year_'+ str(tyear) + '_corr_noise_' + str(corr_noise)+ '.csv'), index=True, header=True)\n",
    "            \n",
    "            sim_num +=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e79ab-b9a5-4f3c-a3b8-7d15120487c3",
   "metadata": {},
   "source": [
    "### We get a total of 9600 additional scenarios covering years from 2017-2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e02f4d3-f7a5-49f7-af2d-4969c7f1a681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
